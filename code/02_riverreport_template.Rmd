---
title: "River Report `r params$df$river[1]`"
output: 
  html_document:
    code_folding: hide
    css: styles.css
params:
  df: NULL
  generate: FALSE
---
```{r, include=FALSE}
source("functions.R")
knitr::opts_chunk$set(fig.width=14, fig.height=7, dpi=300, out.width="100%")
knitr::opts_knit$set(root.dir = normalizePath("."))

set.seed(1)

generate = params$generate

if (generate) {
  df = params$df
  river = df$river[1]
} else if (!generate) {
  df = cop_df |> dplyr::filter(river == "Isar")
  river = "Isar"
}

# TODO: Make filepath dynamic
slopefile_path = "../data/slopes/"
slopes = read.csv2(
  file = paste(slopefile_path, "Isar_Station_Slope.csv", sep = ""),
  sep = ","
) |> 
  dplyr::select(unit, Slope_1, Slope_Percent) |> 
  dplyr::rename(
    slope = Slope_1,
    pslope = Slope_Percent
  ) |> 
  dplyr::mutate(
    slope = as.numeric(slope),
    pslope = as.numeric(pslope)
  )

df = df |> 
  dplyr::left_join(
    y = slopes,
    by = "unit"
  )

# Structure used for VineCopula and gamCopula
assumed_vine_structure =  matrix(
  # 1 - 2 - 3 where: 1:dur, 2:peak, 3:vol
    c(
      1, 0, 0,
      3, 2, 0,
      2, 3, 3
    ),
    nrow = 3, ncol = 3,
    byrow = TRUE
  )

```

```{r}
# Params
n_syn = 5000 # Number of synthetic data generated
# For the copula density to be displayed also towards the edges of the plot, keep the binwidth low.
# I liked 
# density_binwidth = 0.01
# But for now, keep it high so it renders faster
density_binwidth = 1
```


## The data set
`r river`

```{r}
knitr::kable(head(df))
```

Number of observation (i.e. observed years) differs between stations.

```{r}
# Some of the stations have VERY little observations...
nobs_unit = df |> dplyr::summarise(
  n = dplyr::n(),
  .by = unit
) |>
  dplyr::mutate(
    n_for_table = as.factor(dplyr::if_else(n > 30, ">30", as.character(n)))
  )

knitr::kable(table(nobs_unit$n_for_table))
```


## Aggregated view {.tabset}
Scatterplots of observed variables (Marginal affects Scatterplot) and Pseudo-Observations (Relationship only).

### Volume - Duration
```{r}
# Plots
# Volume-Duration
# With marginal distribution
p1 = ggplot(df, aes(x = volume, y = duration_min)) +
  geom_point() +
  facet_wrap(~unit)
# Pobs
p2 = ggplot(df, aes(x = pobs_vol, y = pobs_dur)) +
  geom_point() +
  facet_wrap(~unit)
p1p2 = (p1 | p2)
p1p2
```

### Volume - Peak
```{r}
# Volume-Peak
# W/ marginal
p1 = ggplot(df, aes(x = volume, y = peak)) +
  geom_point() +
  facet_wrap(~unit)
# Pobs (relation without marginal)
p2 = ggplot(df, aes(x = pobs_vol, y = pobs_peak)) +
  geom_point() +
  facet_wrap(~unit)
p1p2 = (p1 | p2)
plot(p1p2)
```


### Duration - Peak
```{r}
# Duration-Peak
# W/ marginal
p1 = ggplot(df, aes(x = duration_min, y = peak)) +
  geom_point() +
  facet_wrap(~unit)
# Pobs
p2 = ggplot(df, aes(x = pobs_dur, y = pobs_peak)) +
  geom_point() +
  facet_wrap(~unit)
p1p2 = (p1|p2)
plot(p1p2)
```


## Correlation table
```{r}
# Correlation table
cor_table = df|> dplyr::summarise(
  n = dplyr::n(),
  tau_vd = cor(volume, duration_min, method = "kendall"),
  p_vd = cor.test(volume, duration_min, method = "kendall")$p.value,
  rej_vd = p_vd < 0.01,
  tau_vp = cor(volume, peak, method = "kendall"),
  p_vp = cor.test(volume, peak, method = "kendall")$p.value,
  rej_vp = p_vp < 0.01,
  tau_dp = cor(duration_min, peak, method = "kendall"),
  p_dp = cor.test(duration_min, peak, method = "kendall")$p.value,
  rej_dp = p_dp < 0.01,
  .by = unit
)
knitr::kable(cor_table)
```


```{r}
# Distribution of dependence structures
cor_table |> tidyr::pivot_longer(
  cols = c("tau_vd", "tau_vp", "tau_dp"),
  names_to = "relation",
  values_to = "correlation"
) |>
ggplot(aes(y = correlation, x = relation)) +
  geom_boxplot()

```
Averages for more than 10 observations
```{r}
means = cor_table |> dplyr::filter(n > 10) |> dplyr::summarise(
  vd_mean = mean(tau_vd),
  vp_mean = mean(tau_vp),
  dp_mean = mean(tau_dp)
)
knitr::kable(means)
```
Mean of smaller correlation
```{r}
mean(c(means$vp_mean, means$dp_mean))
```
Mean of Means
```{r}
mean(c(means$vd_mean, means$vp_mean, means$dp_mean))
```



## Fitted Copulas {.tabset}



```{r}
river_units = (df |> dplyr::summarise(n = dplyr::n(), .by = unit))$unit

```

### NACs {.tabset}
```{r}
nac_fams = list("1" = "Gumbel", "3" = "Clayton", "5" = "Frank")
nacs = lapply(
  river_units,
  function(name){
    fit_nac(mat = df |> dplyr::filter(unit == name) |> dplyr::select(contains("pobs")) |> as.matrix(), families = c(1, 3, 5))
  }
)
names(nacs) = river_units

# Only compare AICs with vine AICs. I do really not care for any NAC plots. MAYBE if AICs of NACs is a lot smaller than for vine. Then it suggests actually NAC strucutre, but I do not expect this since we have 3 different taus...
```


### Vines {.tabset}

```{r}
# Fit Vines
# 

vines = lapply(
  river_units,
  function(name) {
    mat = df |> dplyr::filter(unit == name) |> dplyr::select(contains("pobs")) |> as.matrix() 
    
    vine = VineCopula::RVineCopSelect(data = mat, Matrix = assumed_vine_structure, familyset = c(3, 4, 5))
  }
)
names(vines) = river_units

```

#### Trees

```{r}
# Optiomal plot dimensions
y = ceiling(sqrt(length(river_units)))
x = ceiling(length(river_units) / y)

par(mfrow = c(y, x))
invisible( # Allows lapply to work well with plot
  lapply(
    river_units, 
    function(name) {
      plot(vines[[name]], type = 2, edge.labels = "family-tau", tree = 1)
      title(sub = name)
    }
  )
)
par(mfrow = c(1, 1))

```

#### Contours / synthetic data {.tabset}

My ggplot densities are not correct. No way. But I do not know why. I mean, the contour plots of the VineCopula package actually look different to mine. But no idea why.... Problem for later. yey.

Only based on the vines fit as they obviously fit the data way better. 

```{r}
# Contours
x = seq(from = 0.01, to = 0.99, length.out = 50)
y = seq(from = 0.01, to = 0.99, length.out = 50)
density_grid = expand.grid(x = x, y = y)

plot_dfs = lapply(
  river_units,
  function(name) get_density_values(vines[[name]], density_grid, name)
)
plot_df = plot_dfs |> dplyr::bind_rows()

```


```{r}
# Synthetic data
syn_dfs = lapply(
  river_units,
  function(name) get_synthetic_data(vines[[name]], n_syn, name)
)
syn_df = syn_dfs |> dplyr::bind_rows()
```

```{r}
syn_df$peak = lapply(
  river_units,
  function(name) inverse_ecdf_unitwise(syn = syn_df, syn_col = "pobs_peak", df = df, df_col = "peak", unit_name = name)
) |> unlist()
syn_df$vol= lapply(
  river_units,
  function(name) inverse_ecdf_unitwise(syn = syn_df, syn_col = "pobs_vol", df = df, df_col = "volume", unit_name = name)
) |> unlist()
syn_df$dur= lapply(
  river_units,
  function(name) inverse_ecdf_unitwise(syn = syn_df, syn_col = "pobs_dur", df = df, df_col = "duration_min", unit_name = name)
) |> unlist()
```

##### Volume - Duration

```{r}
# 1: pobs_dur, 2: pobs_peak, 3: pobs_vol
# 1 - 2 - 3
p1 = ggplot() +
  # 1: pobs_dur, 2: pobs_peak, 3: pobs_vol
  geom_contour(data = plot_df |> dplyr::filter(vars == "z13_2"), aes(x = x, y = y, z = density), binwidth = density_binwidth, alpha = .1) + 
  geom_point(data = df, mapping = aes(x = pobs_vol, y = pobs_dur), alpha = .8) +
  facet_wrap(~ unit)
p2 = ggplot() + 
  geom_point(data = syn_df, mapping = aes(x = vol, y = dur), color = "lightblue") + 
  geom_point(data = df, mapping = aes(x = volume, y = duration_min)) + 
  facet_wrap(~ unit)
p1p2 = (p1 | p2)
p1p2
```


##### Volume - Peak

```{r}
# 1: pobs_dur, 2: pobs_peak, 3: pobs_vol
# 1 - 2 - 3
p1 = ggplot() +
  geom_contour(data = plot_df |> dplyr::filter(vars == "z23"), aes(x = x, y = y, z = density), binwidth = density_binwidth, alpha = .1) + 
  geom_point(data = df, mapping = aes(x = pobs_vol, y = pobs_peak), alpha = .8) +
  facet_wrap(~ unit)
p2 = ggplot() + 
  geom_point(data = syn_df, mapping = aes(x = vol, y = peak), color = "lightblue") + 
  geom_point(data = df, mapping = aes(x = volume, y = peak)) + 
  facet_wrap(~ unit)
p1p2 = (p1 | p2)
p1p2
```

##### Duration - Peak

```{r}
# 1: pobs_dur, 2: pobs_peak, 3: pobs_vol
# 1 - 2 - 3
p1 = ggplot() +
  # 1: pobs_dur, 2: pobs_peak, 3: pobs_vol
  geom_contour(data = plot_df |> dplyr::filter(vars == "z12"), aes(x = x, y = y, z = density), binwidth = density_binwidth, alpha = .1) + 
  geom_point(data = df, mapping = aes(x = pobs_dur, y = pobs_peak), alpha = .8) +
  facet_wrap(~ unit)
p2 = ggplot() + 
  geom_point(data = syn_df, mapping = aes(x = dur, y = peak), color = "lightblue") + 
  geom_point(data = df, mapping = aes(x = duration_min, y = peak)) + 
  facet_wrap(~ unit)
p1p2 = (p1 | p2)
p1p2
```


```{r}
# MYYY DENSITIES ARE WROOOOOOOOOOONG I THINK!!!
# Checked: The Bivariate copulas used to grab density values are correct. Checked this using:
# test = function(){
#   browser()
#   contour(vines[["Sylvenstein"]])
# }
# test()
# --> Allows to jump into the source code and check fitted BiCop. For Sylvenstein that is:
# Bivariate copula: Frank (par = 12.72, tau = 0.73) (vine matrix index: [2, 1]) (family integer: 5)
# Bivariate copula: Clayton (par = 1, tau = 0.33) (vine matrix index: [3, 1])  (family integer: 3)
# Bivariate copula: Survival Gumbel (par = 2.03, tau = 0.51) (vine matrix index: [3, 2]) (family int: 14)
# MY DENSITIES ARE CORREEEEEEEEEEEEEEEECT!!!! 
# Package by default plots their densites using normal margins! I plot them on uniform margins
# Compare mine to package when using uniform margins:
# contour(vines[["Sylvenstein"]], margins = "unif")
# But still, the dependence plot look a lot better with normal margins? wtf...
```


```{r}
# Plot the copula densities and sythetic data. Helps me figuring out what is going wrong... I hope?
# Plot them side by side 
```


## Fit comparison

Check if there are any NACs with huge AIC ratio. In generally, just compare confidence in vine selection vs NAC selection by considering the AIC ratio with NAC AIC / Vine AIC. Only if this ratio is super large and small n, the underlying strucutre might actually be NAC which is, due to the small sample size, badly approximated using vines. 

At least for Isar, there is not one concerning AIC ratio. That is, for presentation: We keep in mind, that if AIC ratio is super large favoring NACs for small sample sizes, we should (probably) fit NAC. For any other case, vines are perfectly fine. 
For Isar, there is only one case where NAC AIC is larger than vine AIC, but still close to 1. So all good. 

Thus, NACs are not relevant at all. Thus, plot Contours only for Vines.

Vines just beat NACs in every aspect in our considered case, really. 
```{r}
# Check if NACs are relevant. If not, only plot Vine density contours with obs data 
nac_aic = unlist(
  lapply(
    nacs, 
    function(nac) ll2aic(attr(nac, "logLik")[[1]], p = 2) 
  )
)
vine_aic = unlist(lapply(vines, function(vine) return(vine$AIC)))
aic = data.frame(
  nac_aic = nac_aic,
  vine_aic = vine_aic,
  aic_ratio = nac_aic / vine_aic
)
aic |> 
  ggplot(aes(y = aic_ratio)) + 
  geom_boxplot()
```
### Single Station

Use a single station to showcase what we could do with these fitted copulas: 
- Evaluate probability of last year flood event (P(VOL >= vol, PEAK >= peak, DUR >= dur))
  - Maybe: Set of all vol, peak, dur pairs for which probability is 1; i.e. century flood triplets?
- Conditional on peak, how does bivariate relatioship volume and duration change? 
  - The conditional dependence structure / conditional copula does NOT change because I only fitted simplified copulas. The condition only has an effect of the conditional probabilities fed into the conditional copula. 
That is, a fixed vol and duration value bzw. for fixed u_1 and u_3, u_1|2 and u_3|2 might change for varying peak / u_2 values.
That is, I use the h function (conditional bivariate copula) to determine the conditional probabilities. These probabilities are then plugged into the conditional CDF
- Quantifying slope effect on dependence structure

Okay, fitting a marginal distribution is a lot more involved than I expected. But this is just an example for how to use the copula. I really do not care for complex approaches to density estimation. Thus: 
Freising for Isar looks okay when I use a pretty simple approach. I would like to compare the empirical and parametrical cdf. 
To do that, I just the conditional copula df and then plot that with desired threshold combinations. All I need a quantile function to change the x and y axis labels. 

Thus: copula cdf, mark every combination that says cdf larger than threshold is certain color. Every set of the two variables volume and duration. 

```{r}
if (river == "Isar") station = "München" else station = df$unit[1]
# Freising, Lenggries
# station = "Puppling"
```

Zoomed density and synthetic data

```{r}
# station df 
sdf = df |> dplyr::filter(unit == station)
# station plot_df
splot_df = plot_df |> dplyr::filter(unit == station)
# station synthetic df 
ssyn_df = syn_df |> dplyr::filter(unit == station) 
```

```{r}
colnames = c("peak", "volume", "duration_min")
u = ssyn_df |> dplyr::select(contains("pobs"))
u = u |> dplyr::rename(peak = pobs_peak, volume = pobs_vol, duration_min = pobs_dur)
fits = lapply(
  colnames,
  function(name) invPIT(name, sdf, u[name])
)
names(fits) = colnames

ssyn_df = ssyn_df |>
  dplyr::mutate(
    peakp = fits$peak,
    volp = fits$volume,
    durp = fits$duration_min
  )
```

Evaluation of marginal density fit
```{r}
ggplot() + 
  geom_density(data = sdf, aes(x = volume), color = "blue") + 
  geom_density(data = ssyn_df, aes(x = volp))
ggplot() + 
  geom_density(data = sdf, aes(x = peak), color = "blue") + 
  geom_density(data = ssyn_df, aes(x = peakp))
ggplot() + 
  geom_density(data = sdf, aes(x = duration_min), color = "blue") + 
  geom_density(data = ssyn_df, aes(x = durp))
```




```{r}
# Contour plots
p1 = get_contour("z12", splot_df, sdf, "pobs_dur", "pobs_peak")
p2 = get_contour("z23", splot_df, sdf, "pobs_peak", "pobs_vol")
p3 = get_contour("z13_2", splot_df, sdf, "pobs_dur", "pobs_vol")
# Synthetic data on [0, 1]
p4 = get_syn_scatter(ssyn_df, "pobs_dur", "pobs_peak", sdf, "pobs_dur", "pobs_peak")
p5 = get_syn_scatter(ssyn_df, "pobs_peak", "pobs_vol", sdf, "pobs_peak", "pobs_vol")
p6 = get_syn_scatter(ssyn_df, "pobs_dur", "pobs_vol", sdf, "pobs_dur", "pobs_vol")
# Synthetic data based on empirical invPIT
p7 = get_syn_scatter(ssyn_df, "dur", "peak", sdf, "duration_min", "peak")
p8 = get_syn_scatter(ssyn_df, "peak", "vol", sdf, "peak", "volume")
p9 = get_syn_scatter(ssyn_df, "dur", "vol", sdf, "duration_min", "volume")
# Synthetic data based on parametric invPIT
p10 = get_syn_scatter(ssyn_df, "durp", "peakp", sdf, "duration_min", "peak")
p11 = get_syn_scatter(ssyn_df, "peakp", "volp", sdf, "peak", "volume")
p12 = get_syn_scatter(ssyn_df, "durp", "volp", sdf, "duration_min", "volume")

p = (p1 | p2 | p3) / 
    (p4 | p5 | p6) /
    (p7 | p8 | p9) /
    (p10 | p11 | p12)
  
p
```
Parametric synthetic invPIT looks super bad. But thats okay. The empirical invPIT leads to these bins. Of corse, it is more intersting without those, else the continously drawn copula observations are forced into these bins and invPIT forces intersting thresholds into these bins.
Thus, we used a very simple approach to estimate the marginal empirival density. That fit is not that  great. But that is okay, since we only want to give an example on how to use fitted copula in practice. So: Take results with grain of salt and consider it more of an example. For sound statements, we would require a more sophisticated marginal distribution estimation

The discrapency between the empirical PIT and the parametric PIT is due to the suboptimal fit...
As I said: Grain of salt <3 


The synthetic data looks a bit odd. Reason is that we used the empirical PIT. That is, the synthetic data is being "categorized". 
This limits the power of our analysis, of course, because we cannot really translate from copula to margins bzw. the transformation depends on the number of observed data points. This is the clear downside of the non-parametric approach.. 
What I COULD do is using any margin (bzw. inverse of any margin) of my choice to transform data and synthetic data. 
I mean, I could even estimate distributional parameters of the actual margins and use this estimated distribution ONLY for the inverse function. 
That is, the initial transformation to uniform data is non-parametric. The re-transformation uses estimates on the observed distribution. 
This does not change the number of estimates required for our copula. Because the copula is independent of the re-transofmration process. 

That is, the fit of my copula is dirupted by potential missspecifications of the distributions of the margins. 
We have two uncertainties. 
1) in the fitted copula used to draw synthetic data
2) in the estimation of the parametric distribution
BUT no potential distortion in the copula due to parametrically estimate the margins

I could valide my fit by density plot of re-transformed synthetic data vs density of observed data. Especially tail behavior is intersting.
(But this is basically plotting the fitted marginal distribution and the empirical one).
Of interest tho: Using the joint copula CDF to determine thresholds and transflate them into actual scale values using the parametrically fitted inverse.
("2-step semiparametric workflow", Joe: Chapter 5 discusses multi-stage procedures, including semiparametric rank-based transformations.)

Finding a parametric marginal:
- Choose selection of candidate distributions
- Fit each using MLE 
- Compare fits using AIC / BIC 
or
semiparametric!
Spline based where many data point, parametric where few data points
This is especially interesting because I care about tail behavior and splines tend to be instable at towards the edges of the observed data!
Try evmix package!
But somehow this does not work at all...

Keep this as idea, but use some basic R stuff to proxi the density. This does not need to be super accurate since I focus more on the copula part. This is a possible extension: Improve on the estimating process of the marginals. Just use it and see how it looks.

TODO: 
- Tail dependence analysis
- Implementation of the planned thresholds for probability events

!!! I need MARGINAL density estimates but these do not capture the JOINT behavior. 


#### Last year flood
Use the synthetic data to determine the probability of last years flood event or a more severe flood event
$$
PEAK \geq peak, VOL \geq vol, DUR \geq dur
$$

```{r}
last_flood = sdf |> 
  dplyr::filter(year == max(sdf$year)) |> 
  dplyr::select(year, peak, pobs_peak, duration_min, pobs_dur, volume, pobs_vol)

last_flood
```

That is, we compute the CDF at (`r last_flood$pobs_peak`, `r last_flood$pobs_dur`, `r last_flood$pobs_vol```)

Last years flood in graphs
```{r}
p1 = get_contour("z12", splot_df, sdf, "pobs_dur", "pobs_peak")
p1 + geom_point(data = last_flood, aes(x = pobs_dur, y = pobs_peak), color = "red")

p2 = get_contour("z23", splot_df, sdf, "pobs_peak", "pobs_vol")
p2 + geom_point(data = last_flood, aes(x = pobs_peak, y = pobs_vol), color = "red")

p3 = get_contour("z13_2", splot_df, sdf, "pobs_dur", "pobs_vol")
p3 + geom_point(data = last_flood, aes(x = pobs_dur, y = pobs_vol), color = "red")
```


Probability of such a flood event or a more severe one (Based on simulation)
(Here, I use simulated data because a lot simpler than VineCopula function. I would have to evaluate VineCopula function multiple times (3 times for 2d) to obtain joint $X>=x, Y>=y$)

```{r}
# P(X>x, Y>y, Z>z)
syn_vals = ssyn_df |> dplyr::select(pobs_dur, pobs_peak, pobs_vol)  
thresh_vals = c(last_flood$pobs_dur, last_flood$pobs_peak, last_flood$pobs_vol)
# Idea: Use threshold from last flood
# For every entry in synthetic data, check if value is larger corresponding threshold
# If all three (rowsum) are larger than their thresholds, they are an occurance of X>x, Y>y, Z>z
# Determine frequence of these events
prob = sum(rowSums(syn_vals >= thresh_vals) == 3) / nrow(ssyn_df)
print(prob)  
```
On average, every `r 1 / prob` years such a flood occurs.


#### Jahrhundertfluten
What are the combinations of peak, duation and volume that only have a 1 percent probability. 
Plot Bivariate for all 3
Plot trivariate right underneath for all 3 

To have reference and simplify 3d display: 
Keep peak fixed and consider how volume and duration change for a Jahrhundertflut where the peak is the same as the one we had in the flood in last year 



POSTPONED. Not sure how to give a good viz and I busy anyway

#### Trying coonditional distribution...
Consider the joint CDF for volume and duration for varying values of peak.

```{r}
# Approach:
# Select values for X_2 
# Transform to U_2 bzw. just use empirical quantiles, then I already have the pseudo obs
# Determine U_1|2 and U_3|2 using h function from VineCopula package
# Evaluate conditional copula cdf at u_1|2 and u_3|2
# Return those probabilities
# Repeat the above for a grid of X_1, X_3 values given a certain X_2 and create a contour plot

emp_quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95, 0.99)
peak_idxs = ceiling(quantile(1:nrow(sdf), probs = emp_quantiles)) # Not sure, just using some random values for now
peaks = sdf |> 
  dplyr::arrange(peak) |> 
  dplyr::mutate(row = dplyr::row_number()) |> 
  dplyr::filter(row %in% peak_idxs) 
peaks |> dplyr::select(peak)

peaks$pobs_peak

# Vine models
vine = vines[[station]]

bicop_dp = VineCopula::BiCop(family = vine$family[3, 1], par = vine$par[3, 1])
bicop_pv = VineCopula::BiCop(family = vine$family[3, 2], par = vine$par[3, 2])
bicop_cond_dv = VineCopula::BiCop(family = vine$family[2, 1], par = vine$par[2, 1]) 

# NAC model
nac = nacs[[station]]
```

<!-- ```{r} -->
<!-- ####### Old -->
<!-- # Conditional distribution: -->
<!-- # Draw samples from the two unconditional copulas given a peak value -->
<!-- # i.e. randomly draw u_1 and u_3 given a value for u_2 -->
<!-- # Then, plot these probabilities in a scatter plot. In theory, these points should resemble the conditonal copula? Not sure about that tho. But the conditonal copula represents the dependence between the conditonal probabilities and, according to assumption of simplified vines, this dependence is not influenced by the specific peak value conditioned on.  -->
<!-- # How would I re-transform conditional probabilities into marginal values?  -->
<!-- # Can I just draw the conditional probabilities (conditional on peak value), check dependence (simplified vine assumption). Then, I am interested in the marginal duration bzw. volume values to expect when conditioning on the peak values. So I re-transform the conditional u values into uncoditional setting. Then, I can inverse transform these and I get the duration bzw. volume values (issue is when invPIT conditonal probabilities, they are not really the correct "scale". That is, conditonal probability of 0.9 may refer to a marginal value of a with conditon b or to marginal value c with condition d. Because depending on the condition, the conditonal probability for value a bzw. c changes.) -->
<!-- ######### Old -->
<!-- ``` -->
```{r}
# Evaluate NAC and Vine over grid fixing peak value to obtain triplet (peak, vol, dur) with peak fixed
```


```{r}
n_draws = 10000
peak = peaks$pobs_peak[[3]]
# Use inverse h to obtain unconditional values (Czado p.62)
# That is: 
# 1) Draw n probabilities to condition on u_2i
# 2) Draw another n uniform variables v_i
# 3) Define v_i as conditonal probability: v_i = u_1i|u_2i (as any other probability, it is uniform)
# 4) Use inverse h function: h.inv(v_i|u_2i) = u_1i (UNCONDITIONAL!)
#   -> I can draw (u_1i, u_2i) pairs for a fixed u_2i
#   -> That is, I would not condition on a peak value, but consider the most probable pairs for a fixed peak value? Just like paper I guess (i.e. Considering the triplet (peak, vol, dur) and holding peak fixed at certain point !! Not conditoned in, just holding it fixed!!, I can draw most probable volume-duration pairs: For each fixed peak, I can check the scatterplot / determine point with highest density)
cond_probs = runif(n_draws, 0, 1)
uncon_dur = VineCopula::BiCopHinv1(cond_probs, rep(peak, n_draws), bicop_dp) # hinv1: inverse u_2|u_1 --> gives unconditonal u_1 where u_1 is first argument into function and u_2 second argument

# Considering a full df 
trips = data.frame( # Create a data frame where we look at triplets keeping peak constant 
  # Fixed peaks
  peak = rep(peaks$pobs_peak, each = n_draws),
  cond_prob1 = rep(runif(n_draws, 0, 1), length(peaks$pobs_peak)),
  cond_prob2 = rep(runif(n_draws, 0, 1), length(peaks$pobs_peak))
  # Random peaks
  # peak = runif(n_draws, 0, 1),
  # cond_prob1 = runif(n_draws, 0, 1),
  # cond_prob2 = runif(n_draws, 0, 1)
  ) |> 
  dplyr::mutate(
    dur = VineCopula::BiCopHinv1(cond_prob1, peak, bicop_dp),
    vol = VineCopula::BiCopHinv1(cond_prob2, peak, bicop_pv)
  ) 

trips |> 
  ggplot() + 
  geom_point(aes(y = vol, x = dur, color = peak))

# Most probable point for each peak-value
# Estimate the 2D kernel density
kde <- MASS::kde2d(
  (trips |> dplyr::filter(peak == peaks$pobs_peak[[1]]))$dur, 
  (trips |> dplyr::filter(peak == peaks$pobs_peak[[1]]))$vol, 
   n = 100
)

# Find the mode (highest density)
max_idx <- which(kde$z == max(kde$z), arr.ind = TRUE)
mode = list( 
  x = kde$x[max_idx[1]],
  y = kde$y[max_idx[2]]
)


trips |> 
  ggplot() + 
  geom_point(aes(y = vol, x = dur, color = peak)) + 
  geom_point(data = data.frame(mode), aes(x = x, y = y))

# Mode for NAC
nac = nacs[[station]]
# Evaluate nac density for fixed peak and varying vol and dur over a grid
# Using this grid, select highest density point ~ mode
# Legit, can use the same approach for vine....
### COnditonal NAC: 
# Use Hfunciton from VineCopula:
# Condition of peak again
# For peak, use invH to get values of nested copula
# For the value of nested copula, I get a set of u1 and u2 fulfillling copula is this. I basically need the invCDF of a bivariate copula. Thus, I can again use VineCopulap package for this. 
# I get a set of variables fulfilling the copula == value
# Choose those variable combination that is most likely? 
# Idea: Wurf mit 2 Würfeln hat bestimmte Wkeit unter 6 zu bleiben, aber manche dieser Kombinationen unter 6 zu bleiben sind wahrscheinlicher als andere. Nimm die Kombination, die am Wahrscheinlichsten ist

```
```{r}
# 1) Draw from conditional copula. 
# 2) Use inverse h functions to obtain vol and dur values,
#   That is, plug in any values (like the empirical quantiles values for every conditional synthetic draw) and use inverse h function to obtain the unconditional probability 
# 3) Consider the scatterplot of unconditonal probabilities (Does it look different to before?)
# 4) Given these unconditional probabilities, I can apply invPIT and check out the marginal distribution. This should be the conditional(?) marginal distribution of vol / dur. 
#   Bc: These marginal values are based on draws of the conditonal copula. That is:
#     Conditional on a peak value, this must be the vol bzw. dur value s.t. the drawn conditonal prbability is obtained. That is, given a peak value, I obtain the vol bzw. dur values. 
##  Confusing tho is that I obtain the UNCONDITIONAL vol and dur values. However, they are somewhat conditoned, right? Not sure...
```



<!-- ```{r} -->
<!-- cond_df = lapply( -->
<!--   peaks$pobs_peak, -->
<!--   function(current_peak) { -->
<!--     u_cond_dp = VineCopula::BiCopHfunc2(ssyn_df$pobs_dur, rep(current_peak, nrow(ssyn_df)), bicop_dp) -->
<!--     u_cond_pv = VineCopula::BiCopHfunc2(ssyn_df$pobs_vol, rep(current_peak, nrow(ssyn_df)), bicop_pv) -->

<!--     cond_cdf = VineCopula::BiCopCDF(u_cond_dp, u_cond_pv, bicop_cond_dv) -->

<!--     d = data.frame( -->
<!--       u_cond_dp, -->
<!--       u_cond_pv, -->
<!--       cond_cdf, -->
<!--       current_peak -->
<!--     )  -->
<!--     return(d) -->
<!--   } -->
<!-- ) -->
<!-- cond_df = cond_df |> dplyr::bind_rows() |> dplyr::left_join(sdf |> dplyr::select(pobs_peak, peak) |> dplyr::rename(current_peak = pobs_peak), by = "current_peak") -->

<!-- # u_cond_dp = VineCopula::BiCopHfunc2(ssyn_df$pobs_dur, rep(current_peak, nrow(ssyn_df)), bicop_dp) -->
<!-- # u_cond_pv = VineCopula::BiCopHfunc2(ssyn_df$pobs_vol, rep(current_peak, nrow(ssyn_df)), bicop_pv) -->
<!-- #  -->
<!-- # cond_cdf = VineCopula::BiCopCDF(u_cond_dp, u_cond_pv, bicop_cond_dv) -->
<!-- # # cond_cdf = VineCopula::BiCopPDF(u_cond_dp, u_cond_pv, bicop_cond_dv) -->
<!-- #  -->
<!-- # data.frame( -->
<!-- #   u_cond_dp, -->
<!-- #   u_cond_pv, -->
<!-- #   cond_cdf -->
<!-- # ) |>  -->
<!-- #   ggplot() +  -->
<!-- #   geom_point(aes(x = u_cond_dp, u_cond_pv, color = cond_cdf)) -->
<!-- #   # stat_contour(aes(x = x, y = y, z = cond_cdf), binwidth = 0.01) -->
<!-- cond_df |>  -->
<!--   ggplot() +  -->
<!--   geom_point(aes(x = u_cond_dp, y = u_cond_pv, color = cond_cdf), alpha = 0.7) +  -->
<!--   facet_wrap(~peak) -->
<!-- ``` -->

Expectation: Conditional probability for duration given peak is close to uniform because they look almost indepedent. Thus, conditioning should not change a lot. 
For volume, the dependence in stronger and I expect a larger effect from the condition on a peak value.

Thing is, the damn cdf and pdf are constant. The effect of conditioning on a peak value is that it changes probable pairs i.e. conditioning on a certain peak turn unconditional probabilities in conditional ones. surprise. Due to the condition, the dependence structure remains the same (conditonal copula is constant), but probabilities are adjusted. 
So: the conditional copula remains constant, but the condition changes the attained probabilities.





#### CDF Triplets for which HQ event

I want to find those triplets for which a certain probability holds. 
Or maybe also use condtional distribution? THen I can do this in 2d
Like: Conditional on last years peak, what would be a vol and dur distr. s.t. HQflood



### Tail dependence
Check tail dependence behavior for each variable combination
- Give distribution of copula family by variables !! Keep roration and survival as separate family of course! 
- Give distribution of tail dependence values 
Are some variables highly dependent? 

```{r}
tdeps = lapply(
  river_units,
  function(name) get_tail_dependencies(vines[[name]], name)
)
tdep = tdeps |> dplyr::bind_rows()
knitr::kable(tdep)
```


<!-- ## gamVine -->

<!-- ```{r} -->
<!-- # !!! Must estimate tau! Because only tau is compareable among different copulas.  -->
<!-- # i.e. I am intersted in modelling tau over the different copulas. Not sure if I need gamVine at all for that ... -->
<!-- # Maybe just Fisher z trans and model using GAM? Try that in simulation? -->
<!-- # Issue: gamCopula assumes constant copula structure -->
<!-- # After all, modelling tau as function of a covariate is totally independent of the copula approach.  -->
<!-- # So I would basically mix Li and Nagler? Li: Model tau using Bayesian, Nagler: Model tau using GAM -->
<!-- # Also, I could implement a GAMM with an additional spline component? I mean, I am just more flexible when fitting tau on my own terms -->

<!-- # Let's try: -->
<!-- fisher_z = function(r) 1/2 * log( (1+r) / (1-r) ) -->
<!-- inv_fisher_z = function(z) (exp(2 * z) - 1) / (exp(2 * z) + 1)  -->
<!-- to_beta = function(r) (r + 1) / 2 -->
<!-- from_beta = function(b) 2 * b - 1 -->


<!-- # Issue:  -->
<!-- # Fisher_z does not has a lot of impact for low values of tau -->
<!-- # That is, bimodal distribution of tau close to 0 is barely shifted -->
<!-- # Only consider  -->
<!-- test = cor_table |>  -->
<!--   dplyr::mutate( -->
<!--     z_dp = fisher_z(tau_dp), -->
<!--     z_vp = fisher_z(tau_vp), -->
<!--     z_vd = fisher_z(tau_vd), -->
<!--     b_dp = to_beta(tau_dp), -->
<!--     b_vp = to_beta(tau_vp), -->
<!--     b_vd = to_beta(tau_vd) -->
<!--   )  |>  -->
<!--   dplyr::select( -->
<!--     unit, tau_vd, z_vd, b_vd -->
<!--   )  -->
<!-- names(test) = c("unit", "tau", "z", "b") -->

<!-- test |>  -->
<!--   ggplot() +  -->
<!--   geom_density(aes(x = tau)) +  -->
<!--   geom_density(aes(x = z), color = "red", linetype = 2) +  -->
<!--   geom_density(aes(x = b), color = "blue", linetype = 3)  -->

<!-- ``` -->


<!-- ```{r} -->
<!-- |>  -->
<!--   tidyr::pivot_longer( -->
<!--     cols = c(contains("z"), contains("b"), contains("tau")), -->
<!--     names_to = "vars", -->
<!--     values_to = "z" -->
<!--   )  |>  -->
<!--   dplyr::summarise( -->

<!--     .by = c(unit, z) -->
<!--   ) -->
<!-- test -->
<!-- ``` -->


<!-- ```{r} -->
<!-- ggplot() +  -->
<!--   geom_density(aes(x = z)) +  -->
<!--   facet_wrap(~type) -->


<!-- # Try different links: -->
<!-- # (tau + 1) / 2 = b ~ beta bzw. tau = b * 2 - 1  -->
<!-- # fisher_z ~ norm -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # gamCOpula tries -->
<!-- gam = gamCopula::gamVineCopSelect( -->
<!--   # 1: pobs_dur, 2: pobs_peak, 3: pobs_vol -->
<!--   data = df |> dplyr::select(pobs_dur, pobs_peak, pobs_vol), -->
<!--   Matrix = assumed_vine_structure, -->
<!--   lin.covs = df |> dplyr::select(pslope), -->
<!--   simplified = TRUE -->
<!-- ) -->

<!-- df |>  -->
<!--   dplyr::summarise( -->
<!--     .by = c(unit, pslope) -->
<!--   ) |>  -->
<!--   dplyr::left_join( -->
<!--     # Think z is Fisher-z -->
<!--     gam@model[[1]]@model$model, -->
<!--     by = "pslope" -->
<!--   ) |>  -->
<!--   ggplot() + -->
<!--   geom_density(aes(x = z)) +  -->
<!--   facet_wrap(~unit) -->

<!-- gam@model -->
<!-- gam@model[[1]]@model$coefficients -->
<!-- gam@model[[1]]@model$model -->


<!-- table(gam@model[[1]]@model$fitted.values) -->
<!-- gam@model[[1]]@model$fitted.values -->
<!-- cor_table |> dplyr::select(unit, tau_dp) -->
<!-- ``` -->


